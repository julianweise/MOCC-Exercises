Vorweg sollte erwähnt werden, dass die durchgeführten Tests allesamt auf einer Google Cloud Instanz mit aktivierter "Nested Virtualization" durchgeführt werden.

Durch den zusätzlichen Level an Virtualisierung entsteht ein Mehraufwand, der die Testergebnisse an manchen Stellen beeinflussen kann.
 Der Leser sei an dieser Stelle also gewarnt, dass die Ergebnisse nicht immer vergleichbar mit Ergebnissen auf einem nativen System sind.


1. CPU Benchmark
Im Großen und Ganzen ordnen sich die CPU Performance Ergebnisse unseren Erwartungen unter.
Vor allem die durchweg bessere Leistung der nativen Maschine (in unserem Fall virtualisiert!) hingegen allen anderen Virtualisierungen / Containerisierungen (nested!) war zu erwarten, da in jedem Fall durch Virtualisierung / Containerisierung zusätzlicher Aufwand entsteht (Isolierung, Resourcenmanagement, System-Calls, ...).
Ebenso wenig verwunderlich ist die deutliche (!) Performancedegredierung der QEMU Virtualisierung im Vergleich zur Virtualisierung mit Hardwareunterstützung (KVM).
Ohne gegebene Hardware-Unterstützung müssen sämtliche Instruktionen eine Binary-Translation durchlaufen, welches einen erheblichen Mehraufwand nach sich zieht und unzählige Prozessor-Zyklen beansprucht.
Die CPU-Performance schwindet somit natürlich merklich.

Erstaunt sind wir über die Ähnlichkeit der Leistung von Container- und Virtualisierungslösung mit Hardwareunterstützung.
Obwohl QEMU dank KVM über Hardwareunterstützung verfügt, hätten wir erwartet, dass die Containerisierung auf Grund ihrer Leichtgewichtigkeit und der Ausführung als isolierter Container (App) eine bessere Performance aufweist.
An dieser Stelle könnte jedoch die Nutzung eines vollen Ubuntu-Images (mangelnde Leichtgewichtigkeit) als Base-Image im Container die Erklärung für die nahezu gleiche Performance sein.

Sämtliche Ergebnisse befinden sich auch als Grafik im Anhang als PDF-Datei.

2. Memory Benchmark
Die Ergebniss des Speichervergleichs entsprechen unseren Erwartungen.
Im Vergleich des Ausfürhungsdauer für das memsweep Programm schneided die (nested) virtualisierte Lösung mit QEMU deutlich am schlechtesten ab.
Die restlichen drei Testreihen unterscheiden sich nur marginal.
Dies lässt sich vermutlich darauf zurückführen, dass die Virtualisierungsschicht von Google (für nested virtualization benötigt) besonders gut für "Hardwareunterstützung" optimiert ist.
Benötige Sys-Calls werden vermutlich mit minimalem Overhead direkt an den tatsächlichen Hypervisor der eigentlich Maschine weitergeleitet, sodass sich kaum nennenswerte Unterschiede in unseren Tests ergeben haben.
Lediglich ohne Hardwareunterstützung sind signifikante Unterschiede sichtbar, die sehr schön den möglichen Overhead einer (lediglich) softwaregestützen Virtualisierung demonstrieren.

Sämtliche Ergebnisse befinden sich auch als Grafik im Anhang als PDF-Datei.

3. Disk Benchmark
Die Ergebnisse im Disk Benchmark sind zum einem geprägt durch sehr viele Störungen (noise), zum anderen jedoch auch gespickt mit Überraschungen.
Aus unserer Sicht sind die Messergebnisse nativ nahezu nicht verwertbar, da sie über die gesamte Spanne sehr unregelmäßig und in der relativen Beziehung zu den übrigen Werten nicht erklärbar verlaufen.
An dieser Stelle sei jedoch nochmal erwähnt, dass die Messungen auf einer Google Cloud Instanz durchgeführt wurden und somit nicht absolut verlässlich sind.
Im hinteren Bereich der Ergebnisse bewegen sich die I/O Ergebnisse (Write IOPS) für die Containerisierung ähnlich den Werten vom nativen System.
Dies entspricht unseren Erwartungen.
Dass die Ergebnisse der KVM Instanz sich auf einem ähnlichen Niveau bewegt, wie die Ergebnisse der Messung auf dem nativen System, entspricht ebenso unseren Erwartungen.
Dass sie jedoch darüber liegen (wenn auch nur leicht) ist für uns jedoch nicht erklärbar.
Wir vermuten hier auch Messungenauigkeiten auf Grund der nesten virtualization.
Die schlechte Performance der vollen Virtualisierung mittels QEMU entspricht ebenso unseren Erwartungen.
Auf Grund der Binary Translation, vor allem bei System Calls (nötig für I/O Operationen), leidet die Performance hier.

Sämtliche Ergebnisse befinden sich auch als Grafik im Anhang als PDF-Datei.


3.2 Disk format
Für unsere Tests haben wir bei QEMU auf das Plattenformat qcow2 gesetzt.
Dieses ist der aktuelle Standard für QEMU und ermöglicht dynamische Allozierung von Speicher.
Das bedeutet, dass bei der Definition einer 10GB großen Platte nicht direkt 10GB Speicher auf der Platte der Host-Maschine blockiert werden, sondern nur bei Bedarf (bis zu 10GB) alloziert werden.
Dies benötigt einie Layer auf Redirektion (mit zusätzlichen Metadaten), im Vergleich zum .raw Format.
Daher würde ein I/O Test mit dem raw Format als Grundlage vermutlich geringfügig bessere Performancewerte einspielen.
Das qed Format hingegen wurde zwar als Weiterentwicklung von qcow betrieben, die Verbesserungen waren jedoch maginal und lohnten nicht als eigenständiges Format.
Die geringfügigen Verbesserungen wurden zurück in das qwoc2 Format verwendet (das wir nutzten).
Daher ist bei einem Test mit dem qed Format kein Performance-Unterschied zu erwarten.

4. Fork Benchmark
Die Ergbenisse des Forkvergleichs entsprechen überwiegend unseren Erwartungen.
Die Testreihe der "nativen" Maschine unterscheiden sich kaum von der Virtualisierung mit Docker, während die Messungen mit KVM messbar schlechter sind.
Hier hätten wir erwartet, dass sich die Messungen mehr wie bei den CPU Benchmarks verhalten (sprich kaum Unterschied zwischen DOcker und KVM, wobei die native Maschine messbar besser abschneided).
Ein Grund dafür könnte sein, dass Docker für die Ausführung von einzelnen Prozessen ausgelegt ist.
Unsere Implementierung von forksum spielt Docker dabei wahrscheinlich in die Hände, da nur eine überschaubare Anzahl an Prozessen zu jeder Zeit existiert, sodass die Optimierung von Docker voll greifen kann.
KVM hingegen ist weniger spezialisiert auf die Ausführung von wenigen Prozessen und kann daher nicht mit der Docker Maschine mithalten.
Außerdem sind die Messergebnisse der QEMU Maschine deutlich abgeschlagen, da Sys-Calls über Software emuliert werden müssen.

Sämtliche Ergebnisse befinden sich auch als Grafik im Anhang als PDF-Datei.

4.2 Rump Unikernel
Der Rump Unikernel bietet keine Implementierung für Sys-Calls wie fork(), exec(), mmap() und sigaction() da diese Funktionen das Rump Paradigma zerstören.
Die Entwickler geben in ihrem FAQ (https://github.com/rumpkernel/wiki/wiki/Info:-FAQ) ein Beispiel: Angenommen ein Rump Prozess bedient eine bestimmte IP Adresse.
Nach dem fork()-Aufruf würden dann allerdings zwei Prozesse für die gleiche IP zuständig sein, sodass die Implementierung/Ausführung von fork() für den allgemeinen Anwendungsfall auf dem Rump Unikernel keinen Sinn ergibt.
Daher lässt sich die geforderte Implementierung nicht ohne weiteres auf dem Rump Unikernel ausführen.
Nichtsdestotrotz könnte unsere Implementierung angepasst werden, indem nicht fork() verwendet wird, sondern tatsächlich ein neuer Prozess mit entsprechenden Parametern erzeugt wird.
Zusätzlich müssten dann noch die Filedescriptoren der Pipes an den neuen Prozess übergeben werden, sodass beide Prozesse mittels Pipe kommunizieren können.
Auf diese Art könnte Forksum auch auf dem Rump Unikernel ausgeführt werden.